{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning - dimensionality reduction\n",
    "## Principal component analysis PCA\n",
    "\n",
    "Follow:\n",
    "- _Introduction to Machine Learning_ [Chapter 3](https://github.com/amueller/introduction_to_ml_with_python/blob/master/03-unsupervised-learning.ipynb) **Section 3.4.1 Principal Component Analysis** (p.142-149)\n",
    "- _Python Data Science Handbook_ [Chapter 5 In Depth: Principal Components Analysis](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.09-Principal-Component-Analysis.ipynb) (p.433-436)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to PCA\n",
    "\n",
    "Principal component analysis is a dimensionality reduction method that projects data onto axes preserving highest variance of the data. Projection onto these axis results in creating new features from linear combinations of the original features. The weights are the principal component axis (loadings), the projected features are the principal components (scores)\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Termininology is vast\n",
    "</div>\n",
    "\n",
    "Term overview:  \n",
    "\n",
    "| Text | sklearn | statistics | other | \n",
    "| --- | --- | --- | ---|\n",
    "| Principal (component) axes| `pca.components_` |loadings | eigenvectors of the covariance matrix|\n",
    "| Explained variance |`pca.explained_variance_`| variance explained |eigenvalues of the covariance matrix|\n",
    "| Explained variance ratio |`pca.explained_variance_ratio_`| proportion of variance explained ||\n",
    "| Principal components | `pca.transform(X)`| scores| transformed/projected features|\n",
    "\n",
    "\n",
    "\n",
    "We will follow code from Python Data Science Handbook Chapter 5:[Principal Component Analysis](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.09-Principal-Component-Analysis.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.axis('equal');\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows in `components_` are the principal component axes. Shape is (n_components, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_components = pd.DataFrame(pca.components_, columns=['X0', 'X1'])\n",
    "f, axs = plt.subplots(p_components.shape[0], 1, figsize=(2,4),sharex=True)\n",
    "for i, ax in enumerate(axs):\n",
    "    p_components.loc[i,:].plot.bar(ax=ax)\n",
    "    ax.set_ylabel('PCA{}'.format(i+1))\n",
    "    ax.set_ylim([-1,1])\n",
    "    ax.grid(True)\n",
    "axs[0].set_title('Principal component axes (loadings)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute `explained_variance_` contains the amount of explained variance by each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.DataFrame(pca.explained_variance_, index=['PCA1', 'PCA2']).plot.bar(legend=False, figsize=(4,4))\n",
    "ax.set_title('Explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute `explained_variance_ratio_` contains the proportion of variance explained by each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.DataFrame(pca.explained_variance_ratio_, index=['PCA1', 'PCA2']).plot.bar(legend=False, figsize=(4,4))\n",
    "ax.set_title('Explained variance ratio (R2)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing component axes with length proportional to explained variance\n",
    "\n",
    "PCA finds an axis where variance is maximum. Then, a perpendicular distance with the next maximum variance and so on. In 2D, there are only two axes to be found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "# plot data\n",
    "ax[0].scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v, ax=ax[0])\n",
    "ax[0].axis('equal');\n",
    "ax[0].set(xlabel='X1', ylabel='X2', title='input')\n",
    "\n",
    "# plot principal components\n",
    "X_pca = pca.transform(X)\n",
    "ax[1].scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.2)\n",
    "draw_vector([0, 0], [0, 3*np.sqrt(pca.explained_variance_[1])], ax=ax[1])\n",
    "draw_vector([0, 0], [-3*np.sqrt(pca.explained_variance_[0]), 0], ax=ax[1])\n",
    "ax[1].axis('equal')\n",
    "ax[1].set(xlabel='First principal component', ylabel='Second principal component',\n",
    "          title='principal components (transformed/projected input)');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming data between representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_pca, np.zeros(X_pca.shape))\n",
    "plt.axis('equal');\n",
    "plt.xlabel('First principal component')\n",
    "plt.title('Data projected onto PCA1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return to original space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = pca.inverse_transform(X_pca)\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8)\n",
    "plt.axis('equal');\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.title('Data projected onto PCA1 axis in original space');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: PCA on breast cancer dataset\n",
    "\n",
    "Introduction to Machine Learning with Python [Chapter 3.4.1](https://github.com/amueller/introduction_to_ml_with_python/blob/master/03-unsupervised-learning.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(15, 2, figsize=(10, 20))\n",
    "malignant = cancer.data[cancer.target == 0]\n",
    "benign = cancer.data[cancer.target == 1]\n",
    "\n",
    "ax = axes.ravel()\n",
    "\n",
    "for i in range(30):\n",
    "    _, bins = np.histogram(cancer.data[:, i], bins=50)\n",
    "    ax[i].hist(malignant[:, i], bins=bins, color=mglearn.cm3(0), alpha=.5)\n",
    "    ax[i].hist(benign[:, i], bins=bins, color=mglearn.cm3(2), alpha=.5)\n",
    "    ax[i].set_title(cancer.feature_names[i])\n",
    "    ax[i].set_yticks(())\n",
    "ax[0].set_xlabel(\"Feature magnitude\")\n",
    "ax[0].set_ylabel(\"Frequency\")\n",
    "ax[0].legend([\"malignant\", \"benign\"], loc=\"best\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA with 2 components for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the first two principal components of the data\n",
    "pca = PCA(n_components=2)\n",
    "# fit PCA model to beast cancer data\n",
    "pca.fit(cancer.data)\n",
    "\n",
    "# transform data onto the first two principal components\n",
    "X_pca = pca.transform(cancer.data)\n",
    "print(\"Original shape: {}\".format(str(cancer.data.shape)))\n",
    "print(\"Reduced shape: {}\".format(str(X_pca.shape)))\n",
    "\n",
    "# plot first vs. second principal component, colored by class\n",
    "plt.figure(figsize=(8, 8))\n",
    "mglearn.discrete_scatter(X_pca[:, 0], X_pca[:, 1], cancer.target)\n",
    "plt.legend(cancer.target_names, loc=\"best\")\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.xlabel(\"First principal component\")\n",
    "plt.ylabel(\"Second principal component\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling features\n",
    "Since these features are on different scales, it is likely that features with large variances (worst area) will dominate the first principal component.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cancer.data.std(axis=0), index=cancer.feature_names).plot.bar(legend=False);\n",
    "plt.title('Standard deviation');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying a `StandardScaler` will bring all features onto a standard scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(cancer.data)\n",
    "X_scaled = scaler.transform(cancer.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA with 2 components for visualization - scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the first two principal components of the data\n",
    "pca = PCA(n_components=2)\n",
    "# fit PCA model to beast cancer data\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# transform data onto the first two principal components\n",
    "X_pca = pca.transform(X_scaled)\n",
    "print(\"Original shape: {}\".format(str(X_scaled.shape)))\n",
    "print(\"Reduced shape: {}\".format(str(X_pca.shape)))\n",
    "\n",
    "# plot first vs. second principal component, colored by class\n",
    "plt.figure(figsize=(8, 8))\n",
    "mglearn.discrete_scatter(X_pca[:, 0], X_pca[:, 1], cancer.target)\n",
    "plt.legend(cancer.target_names, loc=\"best\")\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.xlabel(\"First principal component\")\n",
    "plt.ylabel(\"Second principal component\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting PCA component axes (loadings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PCA component axes shape: {}\".format(pca.components_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PCA component axes:\\n{}\".format(pca.components_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bar plot of the principal component axes (loadings). These are the weights that will be used to combine the original features when transformed/projected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_components = pd.DataFrame(pca.components_, columns=cancer.feature_names)\n",
    "f, axs = plt.subplots(p_components.shape[0], 1, figsize=(12,4),sharex=True)\n",
    "for i, ax in enumerate(axs):\n",
    "    p_components.loc[i,:].plot.bar(ax=ax)\n",
    "    ax.set_ylabel('PCA{}'.format(i+1))\n",
    "    ax.set_ylim([-1,1])\n",
    "    ax.grid(True)\n",
    "axs[0].set_title('Principal component axes (loadings)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to interpret more easily, a heat map of the principal component axes (loadings) might help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(pca.components_, cmap='viridis')\n",
    "plt.yticks([0, 1], [\"First axes\", \"Second axes\"])\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(cancer.feature_names)),\n",
    "           cancer.feature_names, rotation=60, ha='left')\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Principal component axes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are looking at the principal component axes using the scaled data. The first component seems to average most of the features, only some error features are close to zero. In the second component, there are positive and negative contributions.\n",
    "\n",
    "Retry the plot using the unscaled data. You should find that only a few features contribute to the largest two variance directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting explained variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.DataFrame(pca.explained_variance_, index=['PCA1', 'PCA2']).plot.bar(legend=False, figsize=(4,4))\n",
    "ax.set_title('Explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.DataFrame(pca.explained_variance_ratio_, index=['PCA1', 'PCA2']).plot.bar(legend=False, figsize=(4,4))\n",
    "ax.set_title('Explained variance ratio (R2)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at how much variance is explained by increasing number of PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_all = PCA()\n",
    "pca_all.fit(X_scaled)\n",
    "plt.plot(np.cumsum(pca_all.explained_variance_ratio_))\n",
    "plt.xlabel('number of principal components')\n",
    "plt.ylabel('variance ratio')\n",
    "plt.title('breast cancer dataset')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is often used to decide how many PCs to keep: As many PCs as are needed to explain, say 95% of the total variance.\n",
    "\n",
    "The more components you need to explain significant variance, the greater the chance that the data would benefit from non-linear methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
