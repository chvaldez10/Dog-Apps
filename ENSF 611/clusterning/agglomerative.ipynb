{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning - Clustering\n",
    "## Agglomerative clustering\n",
    "\n",
    "Follow:\n",
    "- _Introduction to Machine Learning_ [Chapter 3](https://github.com/amueller/introduction_to_ml_with_python/blob/master/03-unsupervised-learning.ipynb) **Section 3.5.2 Agglomerative Clustering** (p.184-188)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Agglomerative clustering\n",
    "code from Introduction to Machine Learning with Python Chapter 3.5.2:[Agglomerative Clustering](https://github.com/amueller/introduction_to_ml_with_python/blob/master/03-unsupervised-learning.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_agglomerative_algorithm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Initially, each point is its own cluster. Then, in each step, the two clusters that are closest are merged. In the first four steps, two single-point clusters are picked and these are joined into two-point clusters. In step 5, one of the two-point clusters is extended to a third point, and so on. In step 9, there are only three clusters remaining. As we specified that we are looking for three clusters, the algorithm then stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(random_state=1)\n",
    "\n",
    "agg = AgglomerativeClustering(n_clusters=3)\n",
    "assignment = agg.fit_predict(X)\n",
    "\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], assignment)\n",
    "plt.legend([\"Cluster 0\", \"Cluster 1\", \"Cluster 2\"], loc=\"best\")\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How clusters are merged\n",
    "We need to define:\n",
    "1. the *distance metric* and \n",
    "2. the *linkage type*\n",
    "\n",
    "The default *distance metric* is *euclidean* distance\n",
    "\n",
    "The following four *linkage types* are implemented in scikit-learn:\n",
    "\n",
    "**ward**  \n",
    "The default choice, ward picks the two clusters to merge such that the variance in the distance metric within all clusters increases the least. This often leads to clusters that are relatively equally sized\n",
    "\n",
    "**average**  \n",
    "average linkage merges the two clusters that have the smallest average distance between all their points\n",
    "\n",
    "**complete**  \n",
    "complete linkage (also known as maximum linkage) merges the two clusters that have the smallest maximum distance between their points\n",
    "\n",
    "**single**  \n",
    "merges the two clusters that have the smallest minimum distance between their points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dendrogram to visualize cluster formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(random_state=0, n_samples=12)\n",
    "\n",
    "agg = AgglomerativeClustering(n_clusters=3)\n",
    "assignment = agg.fit_predict(X)\n",
    "\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], assignment)\n",
    "plt.legend([\"Cluster 0\", \"Cluster 1\", \"Cluster 2\"], loc=\"best\")\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "\n",
    "# Apply the linkage function to the data array X\n",
    "# returns an array that specifies the distances\n",
    "# bridged when performing agglomerative clustering\n",
    "\n",
    "linkage_array = linkage(X, metric='euclidean', method='ward')\n",
    "\n",
    "# Now we plot the dendrogram for the linkage_array containing the distances\n",
    "# between clusters\n",
    "dendrogram(linkage_array)\n",
    "\n",
    "# mark the cuts in the tree that signify two or three clusters\n",
    "ax = plt.gca()\n",
    "bounds = ax.get_xbound()\n",
    "ax.plot(bounds, [7.25, 7.25], '--', c='k')\n",
    "ax.plot(bounds, [4, 4], '--', c='k')\n",
    "\n",
    "ax.text(bounds[1], 7.25, ' two clusters', va='center', fontdict={'size': 15})\n",
    "ax.text(bounds[1], 4, ' three clusters', va='center', fontdict={'size': 15})\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.ylabel(\"Cluster distance\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying our small example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create array, call linkage(), call dendogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = np.array([[0.5, 0.5], [0.5, 1], [1.5, 1.5]])\n",
    "Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_id1, cluster_id2, distance, num_of_elements in resulting cluster\n",
    "la = linkage(Xs, metric='euclidean', method='complete')\n",
    "la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendrogram(la)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing cluster centres\n",
    "Agglomerative clustering does not provide an attribute with cluster centers. We can calculate this ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X, columns=['f1', 'f2'])\n",
    "df['label'] = agg.labels_\n",
    "df['label'] = df['label'].astype('category') #seaborn plots will be nicer\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = df.groupby(by='label').mean()\n",
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "ax = sns.scatterplot(x='f1', y='f2', hue='label', ax=ax, data=df)\n",
    "\n",
    "centers.plot.scatter(x='f1', y='f2', ax=ax, marker='x', s=80, color='black');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative clustering on a k-means failure case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X_varied, y_varied = make_blobs(n_samples=200,\n",
    "                                cluster_std=[1.0, 2.5, 0.5],\n",
    "                                random_state=170)\n",
    "y_pred = KMeans(n_clusters=3, random_state=0).fit_predict(X_varied)\n",
    "\n",
    "mglearn.discrete_scatter(X_varied[:, 0], X_varied[:, 1], y_pred)\n",
    "plt.legend([\"cluster 0\", \"cluster 1\", \"cluster 2\"], loc='best')\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.title(\"K-means\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = AgglomerativeClustering(n_clusters=3)\n",
    "assignment = agg.fit_predict(X_varied)\n",
    "\n",
    "mglearn.discrete_scatter(X_varied[:, 0], X_varied[:, 1], assignment)\n",
    "plt.legend([\"Cluster 0\", \"Cluster 1\", \"Cluster 2\"], loc=\"best\")\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.title(\"Agglomerative\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X, _ = load_wine(return_X_y=True, as_frame=True)\n",
    "X = pd.DataFrame(StandardScaler().fit_transform(X), columns=X.columns)\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dendrogram to cluster the wines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the ward clustering to the data array X\n",
    "# The SciPy ward function returns an array that specifies the distances\n",
    "# bridged when performing agglomerative clustering\n",
    "linkage_array = linkage(X, method='complete')\n",
    "# Now we plot the dendrogram for the linkage_array containing the distances\n",
    "# between clusters\n",
    "dendrogram(linkage_array);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this representation, we see that there is not much distance (y-axis value) between choosing 3 or 4 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dendogram to cluster the features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the ward clustering to the data array X\n",
    "# The SciPy ward function returns an array that specifies the distances\n",
    "# bridged when performing agglomerative clustering\n",
    "linkage_array = linkage(X.transpose(), method='complete')\n",
    "# Now we plot the dendrogram for the linkage_array containing the distances\n",
    "# between clusters\n",
    "dendrogram(linkage_array, labels=list(X.transpose().index.values), orientation='left');\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dendogram to cluster features based on correlation\n",
    "Instead of euclidean distance, spearman correlation is used to calculate *distance*\n",
    "\n",
    "More precisely, distance = 1 - correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/fastai/fastbook/blob/master/utils.py\n",
    "\n",
    "from scipy.cluster import hierarchy as hc\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cluster_columns(df, figsize=(10,6), font_size=12):\n",
    "    corr = np.round(spearmanr(df).correlation, 4) #correlation matrix\n",
    "    # 1-corr is the distance between columns in rank-correlation space.\n",
    "    corr_condensed = hc.distance.squareform(1-corr) # create a condensed distance matrix\n",
    "    # we could probably run the linkage on the correlation matrix directly?\n",
    "    # corr_condensed = corr\n",
    "    z = hc.linkage(corr_condensed, method='average') # calculate hierachical clustering on condensed distance matrix\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    hc.dendrogram(z, labels=df.columns, orientation='left', leaf_font_size=font_size)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_columns(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative clustering with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "agg = AgglomerativeClustering(n_clusters=3)\n",
    "assignment = agg.fit_predict(X)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "X_2D = pca.transform(X)\n",
    "\n",
    "mglearn.discrete_scatter(X_2D[:, 0], X_2D[:, 1], assignment)\n",
    "plt.legend([\"Cluster 0\", \"Cluster 1\", \"Cluster 2\", \"Cluster 3\"], loc=\"best\")\n",
    "plt.xlabel(\"PC 0\")\n",
    "plt.ylabel(\"PC 1\")\n",
    "plt.title(\"calinski-harabasz score {:.1f}\".format(calinski_harabasz_score(X, assignment)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = AgglomerativeClustering(n_clusters=4)\n",
    "assignment = agg.fit_predict(X)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "X_2D = pca.transform(X)\n",
    "\n",
    "mglearn.discrete_scatter(X_2D[:, 0], X_2D[:, 1], assignment)\n",
    "plt.legend([\"Cluster 0\", \"Cluster 1\", \"Cluster 2\", \"Cluster 3\"], loc=\"best\")\n",
    "plt.xlabel(\"PC 0\")\n",
    "plt.ylabel(\"PC 1\")\n",
    "plt.title(\"calinski-harabasz score {:.1f}\".format(calinski_harabasz_score(X, assignment)));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
