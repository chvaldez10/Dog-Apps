{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Text Data\n",
    "\n",
    "\n",
    "Follow _Introduction to Machine Learning_ [Chapter 7](https://github.com/amueller/introduction_to_ml_with_python/blob/master/07-working-with-text-data.ipynb):\n",
    "-  Chapter 7. Working with text data - 7.7 Bag of Words with more than one word (p.329-350)\n",
    "\n",
    "\n",
    "We will look at supervised classification of text features, in particular sentiment analysis. Given a text commenting on a movie, is this a *positive* or a *negative* comment.\n",
    "\n",
    "Similar problems include for example, given an email text, is this *spam* or *legitimate* message.\n",
    "\n",
    "Our classifiers will only accept numerical features as input. Therefore, we have to **transform** text to a **numerical representation** first.\n",
    "\n",
    "We will start by looking at pre-processing techniques using toy data in this lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words\n",
    "Computing the bag-of-words representation for a **corpus** of **documents** consists of the following three steps:\n",
    "\n",
    "1. Tokenization. Split each document into the words that appear in it (called tokens), for example by splitting them on whitespace and punctuation.\n",
    "\n",
    "2. Vocabulary building. Collect a vocabulary of all words that appear in any of the documents, and number them (say, in alphabetical order).\n",
    "\n",
    "3. Encoding. For each document, count how often each of the words in the vocabulary appear in this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bards_words =[\"The fool doth think he is wise,\",\n",
    "              \"but the wise man knows himself to be a fool\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bards_words` is a corpus.\n",
    "\n",
    "`\"The fool doth think he is wise,\"` is a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(bards_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "print(\"Vocabulary content:\\n {}\".format(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = vect.transform(bards_words)\n",
    "print(\"bag_of_words: {}\".format(repr(bag_of_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dense representation of bag_of_words:\\n{}\".format(\n",
    "    bag_of_words.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(bag_of_words.toarray(), columns=vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the tokenization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = vect.build_analyzer()\n",
    "print(bards_words[0])\n",
    "print(tokenizer(bards_words[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"I am sure that this is   an #awesome hyper-text at https:\\\\ucalgary.ca. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example with repeating words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeating_words = [\"The sun, the sun, shines so bright, so bright.\",\n",
    "                   \"The moon, the moon, reflects so bright, so bright.\"]\n",
    "vect = CountVectorizer()\n",
    "vect.fit(repeating_words)\n",
    "print(vect.vocabulary_)\n",
    "bag_of_words = vect.transform(repeating_words)\n",
    "pd.DataFrame(bag_of_words.toarray(), columns=vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Bag-of-words: min_df\n",
    "> One way to cut back on these is to only use tokens that appear in at least two documents (or at least five documents, and so on). A token that appears only in a single document is unlikely to appear in the test set and is therefore not helpful. We can set the minimum number of documents a token needs to appear in with the min_df parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=2)\n",
    "vect.fit(bards_words)\n",
    "bag_of_words = vect.transform(bards_words)\n",
    "pd.DataFrame(bag_of_words.toarray(), columns=vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Bag-of-words: removing stopwords\n",
    ">Another way that we can get rid of uninformative words is by discarding words that are too frequent to be informative. There are two main approaches: using a language-specific list of stopwords, or discarding words that appear too frequently. scikit-learn has a built-in list of English stopwords in the feature_extraction.text module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "print(\"Number of stop words: {}\".format(len(ENGLISH_STOP_WORDS)))\n",
    "print(\"Every 10th stopword:\\n{}\".format(list(ENGLISH_STOP_WORDS)[::10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words=\"english\")\n",
    "vect.fit(bards_words)\n",
    "bag_of_words = vect.transform(bards_words)\n",
    "pd.DataFrame(bag_of_words.toarray(), columns=vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf\n",
    "[Sklearn Doc - TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)\n",
    "\n",
    ">Instead of dropping features that are deemed unimportant, another approach is to rescale features by how informative we expect them to be. One of the most common ways to do this is using the term frequency–inverse document frequency (tf–idf) method. \n",
    "\n",
    ">The intuition of this method is to give high weight to any term that appears often in a particular document, but not in many documents in the corpus. \n",
    "\n",
    ">If a word appears often in a particular document, but not in very many documents, it is likely to be very descriptive of the content of that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer()\n",
    "vect.fit(bards_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "print(\"Vocabulary content:\\n {}\".format(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_words = vect.transform(bards_words)\n",
    "print(\"tfidf_words: {}\".format(repr(tfidf_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dense representation of tfidf_words:\\n{}\".format(\n",
    "    tfidf_words.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(tfidf_words.toarray(), columns=vect.get_feature_names_out())\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example with repeating words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeating_words = [\"The sun, the sun, shines so bright, so bright.\",\n",
    "                   \"The moon, the moon, reflects so bright, so bright.\"]\n",
    "vect = TfidfVectorizer()\n",
    "vect.fit(repeating_words)\n",
    "tfidf_words = vect.transform(repeating_words)\n",
    "pd.DataFrame(tfidf_words.toarray(), columns=vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeating_words = [\"The sun, the sun, shines so bright, so bright.\",\n",
    "                   \"The moon, the moon, reflects so bright, so bright.\"]\n",
    "vect = TfidfVectorizer(stop_words=\"english\")\n",
    "vect.fit(repeating_words)\n",
    "tfidf_words = vect.transform(repeating_words)\n",
    "pd.DataFrame(tfidf_words.toarray(), columns=vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words with More Than One Word (n-Grams)\n",
    ">One of the main disadvantages of using a bag-of-words representation is that word order is completely discarded. Therefore, the two strings “it’s bad, not good at all” and “it’s good, not bad at all” have exactly the same representation, even though the meanings are inverted. Putting “not” in front of a word is only one example (if an extreme one) of how context matters. \n",
    "\n",
    ">Fortunately, there is a way of capturing context when using a bag-of-words representation, by not only considering the counts of single tokens, but also the counts of pairs or triplets of tokens that appear next to each other. \n",
    "\n",
    ">Pairs of tokens are known as bigrams, triplets of tokens are known as trigrams, and more generally sequences of tokens are known as n-grams. \n",
    "\n",
    ">We can change the range of tokens that are considered as features by changing the ngram_range parameter of CountVectorizer or TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bards_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\n",
    "print(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(cv.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(2, 2)).fit(bards_words)\n",
    "print(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(cv.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 2)).fit(bards_words)\n",
    "print(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(cv.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example with repeating words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeating_words = [\"The sun, the sun, shines so bright, so bright.\",\n",
    "                   \"The moon, the moon, reflects so bright, so bright.\"]\n",
    "cv = CountVectorizer(ngram_range=(1, 2)).fit(repeating_words)\n",
    "print(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(cv.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_words = cv.transform(repeating_words)\n",
    "pd.DataFrame(ngram_words.toarray(), columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Td-idf n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeating_words = [\"The sun, the sun, shines so bright, so bright.\",\n",
    "                   \"The moon, the moon, reflects so bright, so bright.\"]\n",
    "vect = TfidfVectorizer(ngram_range=(1,2))\n",
    "vect.fit(repeating_words)\n",
    "tfidf_words = vect.transform(repeating_words)\n",
    "pd.DataFrame(tfidf_words.toarray(), columns=vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Td-idf n-gram no stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeating_words = [\"The sun, the sun, shines so bright, so bright.\",\n",
    "                   \"The moon, the moon, reflects so bright, so bright.\"]\n",
    "vect = TfidfVectorizer(ngram_range=(1,2), stop_words=\"english\")\n",
    "vect.fit(repeating_words)\n",
    "tfidf_words = vect.transform(repeating_words)\n",
    "pd.DataFrame(tfidf_words.toarray(), columns=vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
