{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and scaling\n",
    "Follow _Introduction to Machine Learning_  \n",
    "- [Chapter 3](https://github.com/amueller/introduction_to_ml_with_python/blob/master/03-unsupervised-learning.ipynb) Section 3.3 Preprocessing and Scaling\n",
    "\n",
    "\n",
    "Some algorithms are sensitive to the scaling of numerical features:\n",
    "- PCA\n",
    "- SVM\n",
    "- Neural networks\n",
    "- Regularized models\n",
    "\n",
    "Preprocessing is used to bring numerical features to a similar scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different scalers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_scaling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. StandardScaler: Mean of zero, variance of one\n",
    "2. MinMaxScaler: Minimum of zero, maximum of one\n",
    "3. RobustScaler: Median of zero, interquartile range of one\n",
    "4. Normalizer: Each sample (row) has unit norm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html\n",
    "\n",
    "# original row [4, 1, 2, 2]\n",
    "# transformed row has length 1\n",
    "row = np.array([0.8, 0.2, 0.4, 0.4])\n",
    "np.sqrt(np.sum(row**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(cancer.data, cancer.target,\n",
    "                                                    random_state=1)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using a minmax scaler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "# print dataset properties before and after scaling\n",
    "print(\"transformed shape: {}\".format(X_train_scaled.shape))\n",
    "print(\"per-feature minimum before scaling:\\n {}\".format(X_train.min(axis=0)))\n",
    "print(\"per-feature maximum before scaling:\\n {}\".format(X_train.max(axis=0)))\n",
    "print(\"per-feature minimum after scaling:\\n {}\".format(\n",
    "    X_train_scaled.min(axis=0)))\n",
    "print(\"per-feature maximum after scaling:\\n {}\".format(\n",
    "    X_train_scaled.max(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform the validation data with the _trained_ scaler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform validation data\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "# print validation data properties after scaling\n",
    "print(\"per-feature minimum after scaling:\\n{}\".format(X_val_scaled.min(axis=0)))\n",
    "print(\"per-feature maximum after scaling:\\n{}\".format(X_val_scaled.max(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important: Scalers are learned - do not call fit with validation/test data\n",
    "\n",
    "Else the data is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "# make synthetic data\n",
    "X, _ = make_blobs(n_samples=50, centers=5, random_state=4, cluster_std=2)\n",
    "# split it into training and Validation sets\n",
    "X_train, X_val = train_test_split(X, random_state=5, test_size=.1)\n",
    "\n",
    "# plot the training and Validation sets\n",
    "fig, axes = plt.subplots(1, 3, figsize=(13, 4))\n",
    "axes[0].scatter(X_train[:, 0], X_train[:, 1],\n",
    "                color=mglearn.cm2(0), label=\"Training set\", s=60)\n",
    "axes[0].scatter(X_val[:, 0], X_val[:, 1], marker='^',\n",
    "                color=mglearn.cm2(1), label=\"Validation set\", s=60)\n",
    "axes[0].legend(loc='upper left')\n",
    "axes[0].set_title(\"Original Data\")\n",
    "\n",
    "# scale the data using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# visualize the properly scaled data\n",
    "axes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n",
    "                color=mglearn.cm2(0), label=\"Training set\", s=60)\n",
    "axes[1].scatter(X_val_scaled[:, 0], X_val_scaled[:, 1], marker='^',\n",
    "                color=mglearn.cm2(1), label=\"Validation set\", s=60)\n",
    "axes[1].set_title(\"Scaled Data\")\n",
    "\n",
    "# rescale the Validation set separately\n",
    "# so Validation set min is 0 and Validation set max is 1\n",
    "# DO NOT DO THIS! For illustration purposes only.\n",
    "val_scaler = MinMaxScaler()\n",
    "val_scaler.fit(X_val)\n",
    "X_val_scaled_badly = val_scaler.transform(X_val)\n",
    "\n",
    "# visualize wrongly scaled data\n",
    "axes[2].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n",
    "                color=mglearn.cm2(0), label=\"training set\", s=60)\n",
    "axes[2].scatter(X_val_scaled_badly[:, 0], X_val_scaled_badly[:, 1],\n",
    "                marker='^', color=mglearn.cm2(1), label=\"Validation set\", s=60)\n",
    "axes[2].set_title(\"Improperly Scaled Data\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Feature 0\")\n",
    "    ax.set_ylabel(\"Feature 1\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side note: `fit_transform()` for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# calling fit and transform in sequence (using method chaining)\n",
    "X_scaled = scaler.fit(X_train).transform(X_train)\n",
    "# same result, but more efficient computation\n",
    "X_scaled_d = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of scaling on supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Without feature scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(cancer.data, cancer.target,\n",
    "                                                    random_state=0)\n",
    "\n",
    "svm = SVC(C=100)\n",
    "svm.fit(X_train, y_train)\n",
    "print(\"Validation set accuracy: {:.2f}\".format(svm.score(X_val, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With MinMax feature scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing using 0-1 scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# learning an SVM on the scaled training data\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# scoring on the scaled Validation set\n",
    "print(\"Scaled Validation set accuracy: {:.2f}\".format(\n",
    "    svm.score(X_val_scaled, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With Standard feature scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing using zero mean and unit variance scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# learning an SVM on the scaled training data\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# scoring on the scaled Validation set\n",
    "print(\"SVM validation accuracy: {:.2f}\".format(svm.score(X_val_scaled, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise: apply feature scaling to energy dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_regressor_mse(model, X, y, scaler):\n",
    "    '''Calculate train and validation mean-squared error (mse) of regressor (model)\n",
    "        \n",
    "        Splits feature matrix X and target vector y \n",
    "        with sklearn train_test_split() and random_state=956.\n",
    "        \n",
    "        Applies a sklearn scaler\n",
    "        \n",
    "        model (sklearn regressor): Regressor to train and evaluate\n",
    "        X (numpy.array or pandas.DataFrame): Feature matrix\n",
    "        y (numpy.array or pandas.Series): Target vector\n",
    "        \n",
    "        returns: training mse, validation mse\n",
    "    \n",
    "    '''\n",
    "    # Split first\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=956)\n",
    "    \n",
    "    # TODO: Learn scaler on training set\n",
    "    \n",
    "    # TODO: Transform validation set with learned scaler\n",
    "    \n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_val_pred = model.predict(X_val_scaled)\n",
    "\n",
    "    return mean_squared_error(y_train, y_train_pred), mean_squared_error(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.datasets import load_energy\n",
    "\n",
    "X, y = load_energy()\n",
    "print('X.shape={}, type(X)={}'.format(X.shape, type(X)))\n",
    "print('y.shape={}, type(y)={}'.format(y.shape, type(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "models = [LinearRegression(), RandomForestRegressor(random_state=88)]\n",
    "\n",
    "# TODO: add a list of scalers to iterate\n",
    "\n",
    "\n",
    "# Iterate all models\n",
    "for model in models:\n",
    "    # Iterate all scalers\n",
    "    for scaler in scalers:\n",
    "        train, val = get_regressor_mse(model, X, y, scaler)\n",
    "        print(\"{} - {}, train_score={:.1f}, validation_score={:.1f}\".format(model.__class__.__name__,\n",
    "                                                                            scaler.__class__.__name__,\n",
    "                                                                                    train,\n",
    "                                                                                    val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling does not seem to make a difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
