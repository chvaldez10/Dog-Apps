{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning - Clustering\n",
    "\n",
    "## KMeans clustering failures and finding k\n",
    "\n",
    "Follow:\n",
    "- _Introduction to Machine Learning_ [Chapter 3](https://github.com/amueller/introduction_to_ml_with_python/blob/master/03-unsupervised-learning.ipynb) **Section 3.5.1 k-Means Clustering**\n",
    "- _Practical Statistics for Data Scientists_ [Chapter 7](https://github.com/gedeck/practical-statistics-for-data-scientists/blob/master/python/notebooks/Chapter%207%20-%20Unsupervised%20Learning.ipynb) Selecting the number of clusters \n",
    "- _Introduction to Machine Learning_ [Chapter 3](https://github.com/amueller/introduction_to_ml_with_python/blob/master/03-unsupervised-learning.ipynb) **Section 3.5.4 Comparing and Evaluating Clustering Algorithms** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failures of k-means\n",
    "\n",
    "> k-means also assumes that all clusters have the same “diameter” in some sense; it always draws the boundary between clusters to be exactly in the middle between the cluster centers. \n",
    "\n",
    "code from Introduction to Machine Learning with Python Chapter 3.5.1:[K-means Clustering](https://github.com/amueller/introduction_to_ml_with_python/blob/master/03-unsupervised-learning.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X_varied, y_varied = make_blobs(n_samples=200,\n",
    "                                cluster_std=[1.0, 2.5, 0.5],\n",
    "                                random_state=170)\n",
    "y_pred = KMeans(n_clusters=3, random_state=0, n_init=10).fit_predict(X_varied)\n",
    "\n",
    "mglearn.discrete_scatter(X_varied[:, 0], X_varied[:, 1], y_pred)\n",
    "plt.legend([\"cluster 0\", \"cluster 1\", \"cluster 2\"], loc='best')\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">One might have expected the dense region in the lower left to be the first cluster, the dense region in the upper right to be the second, and the less dense region in the center to be the third. Instead, both cluster 0 and cluster 1 have some points that are far away from all the other points in these clusters that “reach” toward the center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some random cluster data\n",
    "X, y = make_blobs(random_state=170, n_samples=600)\n",
    "rng = np.random.RandomState(74)\n",
    "\n",
    "# transform the data to be stretched\n",
    "transformation = rng.normal(size=(2, 2))\n",
    "X = np.dot(X, transformation)\n",
    "\n",
    "# cluster the data into three clusters\n",
    "kmeans = KMeans(n_clusters=3, n_init=10)\n",
    "kmeans.fit(X)\n",
    "y_pred = kmeans.predict(X)\n",
    "\n",
    "# plot the cluster assignments and cluster centers\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], kmeans.labels_, markers='o')\n",
    "mglearn.discrete_scatter(\n",
    "    kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], [0, 1, 2],\n",
    "    markers='^', markeredgewidth=2)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> k-means also assumes that all directions are equally important for each cluster. The above plot shows a two-dimensional dataset where there are three clearly separated parts in the data. However, these groups are stretched toward the diagonal. As k-means only considers the distance to the nearest cluster center, it can’t handle this kind of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate synthetic two_moons data (with less noise this time)\n",
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "\n",
    "# cluster the data into two clusters\n",
    "kmeans = KMeans(n_clusters=2, n_init=10)\n",
    "kmeans.fit(X)\n",
    "y_pred = kmeans.predict(X)\n",
    "\n",
    "# plot the cluster assignments and cluster centers\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm2, s=60, edgecolor='k')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
    "            marker='^', c=[mglearn.cm2(0), mglearn.cm2(1)], s=100, linewidth=2,\n",
    "            edgecolor='k')\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Here, we would hope that the clustering algorithm can discover the two half-moon shapes. However, this is not possible using the k-means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many clusters?\n",
    "\n",
    "**Elbow method:** Find point with maximum curverature in the score plot.\n",
    "\n",
    "Different scores:\n",
    "1. Distortion: The sum of squared distances from each point to its assigned center\n",
    "2. [Silhouette](https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient): Mean Silhouette Coefficient of all samples  \n",
    "The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.\n",
    "3. [Calinski-Harabasz](https://scikit-learn.org/stable/modules/clustering.html#calinski-harabasz-index): The ratio of dispersion between and within clusters (higher is better)\n",
    "\n",
    "These scores can also be used to compare clustering algorithms. However, each score will favour different types of clustering algorithms. See advantages and disadvantages in the scikit-learn documentation of the scores.\n",
    "\n",
    "**Important:** Often, the number of clusters is chosen based on the application area, prior knowledge, or business goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X_varied, y_varied = make_blobs(n_samples=200,\n",
    "                                cluster_std=[1.0, 2.5, 0.5],\n",
    "                                random_state=170)\n",
    "y_pred = KMeans(n_clusters=3, random_state=0, n_init=10).fit_predict(X_varied)\n",
    "\n",
    "mglearn.discrete_scatter(X_varied[:, 0], X_varied[:, 1], y_pred)\n",
    "plt.legend([\"cluster 0\", \"cluster 1\", \"cluster 2\"], loc='best')\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distortion (`kmeans.inertia_`)\n",
    "code from Practical Statistics for Data Scientists [Unsupervised Learning](https://github.com/gedeck/practical-statistics-for-data-scientists/blob/master/python/notebooks/Chapter%207%20-%20Unsupervised%20Learning.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = []\n",
    "for n_clusters in range(2, 14):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=10).fit(X_varied)\n",
    "    inertia.append(kmeans.inertia_ )\n",
    "    \n",
    "inertias = pd.DataFrame({'n_clusters': range(2, 14), 'inertia': inertia})\n",
    "ax = inertias.plot(x='n_clusters', y='inertia')\n",
    "plt.xlabel('Number of clusters(k)')\n",
    "plt.ylabel('Within-Cluster Squared Distances')\n",
    "plt.ylim((0, 1.1 * inertias.inertia.max()))\n",
    "plt.grid(True)\n",
    "ax.legend().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "sils = []\n",
    "n_clusters_list = range(2, 14)\n",
    "\n",
    "for n_clusters in n_clusters_list:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=4, n_init=10).fit(X_varied)\n",
    "    sils.append(silhouette_score(X_varied, kmeans.labels_ ))\n",
    "\n",
    "    \n",
    "df_sils = pd.DataFrame({'n_clusters': n_clusters_list, 'silhouette': sils})\n",
    "ax = df_sils.plot(x='n_clusters', y='silhouette')\n",
    "\n",
    "plt.xlabel('Number of clusters(k)')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.ylim((0, 1.1 * df_sils.silhouette.max()))\n",
    "plt.grid(True)\n",
    "ax.legend().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calinski-Harabasz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "chs = []\n",
    "n_clusters_list = range(2, 14)\n",
    "\n",
    "for n_clusters in n_clusters_list:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=4, n_init=10).fit(X_varied)\n",
    "    chs.append(calinski_harabasz_score(X_varied, kmeans.labels_ ))\n",
    "\n",
    "    \n",
    "df_chs = pd.DataFrame({'n_clusters': n_clusters_list, 'calinski_harabasz': chs})\n",
    "ax = df_chs.plot(x='n_clusters', y='calinski_harabasz')\n",
    "\n",
    "plt.xlabel('Number of clusters(k)')\n",
    "plt.ylabel('Calinski-Harabasz score')\n",
    "plt.ylim((0, 1.1 * df_chs.calinski_harabasz.max()))\n",
    "plt.grid(True)\n",
    "ax.legend().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Elbow using yellowbrick\n",
    "See yellowbrick documentation for [Elbow method](https://www.scikit-yb.org/en/latest/api/cluster/elbow.html#) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster.elbow import kelbow_visualizer\n",
    "ke=kelbow_visualizer(KMeans(random_state=4, n_init=10), X_varied, k=(2,10), \n",
    "                  metric='distortion',\n",
    "                 timings=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster.elbow import kelbow_visualizer\n",
    "ke=kelbow_visualizer(KMeans(random_state=4, n_init=10), X_varied, k=(2,10), \n",
    "                  metric='silhouette',\n",
    "                 timings=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster.elbow import kelbow_visualizer\n",
    "ke=kelbow_visualizer(KMeans(random_state=4, n_init=10), X_varied, k=(2,10), \n",
    "                  metric='calinski_harabasz',\n",
    "                 timings=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding k for the Wine dataset\n",
    "\n",
    "Using these curves to find the optimal k for the wine dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "df, _ = load_wine(return_X_y=True, as_frame=True)\n",
    "df_scaled = StandardScaler().fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster.elbow import kelbow_visualizer\n",
    "ke=kelbow_visualizer(KMeans(random_state=54, n_init=10), df_scaled, k=(2,10), \n",
    "                  metric='distortion',\n",
    "                 timings=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster.elbow import kelbow_visualizer\n",
    "ke=kelbow_visualizer(KMeans(random_state=54, n_init=10), df_scaled, k=(2,10), \n",
    "                  metric='silhouette',\n",
    "                 timings=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster.elbow import kelbow_visualizer\n",
    "ke=kelbow_visualizer(KMeans(random_state=54, n_init=10), df_scaled, k=(2,10), \n",
    "                  metric='calinski_harabasz',\n",
    "                 timings=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- k=3 for Distortion and Silhouette \n",
    "- k=2 for calinski_harabasz. Note that maximum occurs at k=3\n",
    "\n",
    "We know, that there are 3 groups in the wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
