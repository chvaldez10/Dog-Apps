{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "141a1a41",
   "metadata": {},
   "source": [
    "# Extra k-Means examples\n",
    "\n",
    "From *Python Data Science Handbook* Chapter 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc399c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37d6cad",
   "metadata": {},
   "source": [
    "## Handwritten digits\n",
    "\n",
    "We can repeat the digits example that we've used previously to see if k-Means can be used to cluster the different digits together without knowledge of the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b4439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecfde81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=10, random_state=0)\n",
    "clusters = kmeans.fit_predict(digits.data)\n",
    "kmeans.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159518cb",
   "metadata": {},
   "source": [
    "The result is 10 clusters in 64 dimensions. Notice that the cluster centers themselves are 64-dimensional points, and can themselves be interpreted as the “typical” digit within the cluster. Let’s see what these cluster centers look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c303729",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 5, figsize=(8, 3))\n",
    "centers = kmeans.cluster_centers_.reshape(10, 8, 8)\n",
    "for axi, center in zip(ax.flat, centers):\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.imshow(center, interpolation='nearest', cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26446db0",
   "metadata": {},
   "source": [
    "We see that even without the labels, `KMeans` is able to find clusters whose centers are recognizable digits, with perhaps the exception of 1 and 8.\n",
    "\n",
    "Because k-means knows nothing about the identity of the cluster, the 0–9 labels may be in the wrong order. We can fix this by matching each learned cluster label with the true labels found in them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a933c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "labels = np.zeros_like(clusters)\n",
    "for i in range(10):\n",
    "    mask = (clusters == i)\n",
    "    labels[mask] = mode(digits.target[mask], keepdims=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f17fa2b",
   "metadata": {},
   "source": [
    "Now we can check how accurate our unsupervised clustering was in finding similar digits within the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c973ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(digits.target, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfc97f2",
   "metadata": {},
   "source": [
    "With just a simple k-means algorithm, we discovered the correct grouping for almost 80% of the input digits! Let’s check the confusion matrix for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf3bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(digits.target, labels)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=digits.target_names,\n",
    "            yticklabels=digits.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c95150",
   "metadata": {},
   "source": [
    "As we might expect from the cluster centers we visualized before, the main point of confusion is between the eights and ones. This still shows that by using k-means, we can essentially build a digit classifier without reference to any known labels!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601d37fa",
   "metadata": {},
   "source": [
    "Let’s try to push this even further. We can use the t-distributed stochastic neighbor embedding (t-SNE) algorithm to preprocess the data before performing k-means. t-SNE is a nonlinear embedding algorithm that is particularly adept at preserving points within clusters. Let’s see how it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5747175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Project the data: this step will take several seconds\n",
    "tsne = TSNE(n_components=2, init='pca', random_state=0)\n",
    "digits_proj = tsne.fit_transform(digits.data)\n",
    "\n",
    "# Compute the clusters\n",
    "kmeans = KMeans(n_clusters=10, random_state=0)\n",
    "clusters = kmeans.fit_predict(digits_proj)\n",
    "\n",
    "# Permute the labels\n",
    "labels = np.zeros_like(clusters)\n",
    "for i in range(10):\n",
    "    mask = (clusters == i)\n",
    "    labels[mask] = mode(digits.target[mask], keepdims=True)[0]\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy_score(digits.target, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72993568",
   "metadata": {},
   "source": [
    "That’s over 94% classification accuracy without using the labels! This is the power of unsupervised learning when used carefully: it can extract information from the dataset that it might be difficult to do visually or by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6be17a6",
   "metadata": {},
   "source": [
    "## Color compression\n",
    "\n",
    "One interesting application of clustering is in color compression within images. For example, imagine you have an image with millions of colors. In most images, a large number of the colors will be unused, and many of the pixels in the image will have similar or even identical colors.\n",
    "\n",
    "Consider the example below to show how k-Means can be used for color compression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc434a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this requires the pillow package to be installed - I was able to run it with base Anaconda package\n",
    "from sklearn.datasets import load_sample_image\n",
    "china = load_sample_image(\"china.jpg\")\n",
    "ax = plt.axes(xticks=[], yticks=[])\n",
    "ax.imshow(china);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae8e71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "china.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fc49bc",
   "metadata": {},
   "source": [
    "One way we can view this set of pixels is as a cloud of points in a three-dimensional color space. We will reshape the data to `[n_samples x n_features]`, and rescale the colors so that they lie between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e81b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = china / 255.0 # use 0...1 scale\n",
    "data = data.reshape(427 * 640, 3)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60882b38",
   "metadata": {},
   "source": [
    "We can visualize these pixels in this color space, using a subset of 10,000 pixels for efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51405f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pixels(data, title, colors=None, N=10000):\n",
    "    if colors is None:\n",
    "        colors = data\n",
    "\n",
    "    # choose a random subset\n",
    "    rng = np.random.RandomState(0)\n",
    "    i = rng.permutation(data.shape[0])[:N]\n",
    "    colors = colors[i]\n",
    "    R, G, B = data[i].T\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    ax[0].scatter(R, G, color=colors, marker='.')\n",
    "    ax[0].set(xlabel='Red', ylabel='Green', xlim=(0, 1), ylim=(0, 1))\n",
    "\n",
    "    ax[1].scatter(R, B, color=colors, marker='.')\n",
    "    ax[1].set(xlabel='Red', ylabel='Blue', xlim=(0, 1), ylim=(0, 1))\n",
    "\n",
    "    fig.suptitle(title, size=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68db75dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pixels(data, title='Input color space: 16 million possible colors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219c12d8",
   "metadata": {},
   "source": [
    "Now let’s reduce these 16 million colors to just 16 colors, using a k-means clustering across the pixel space. Because we are dealing with a very large dataset, we will use the mini batch k-means, which operates on subsets of the data to compute the result much more quickly than the standard k-means algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431d15c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "kmeans = MiniBatchKMeans(16, batch_size=5120)\n",
    "kmeans.fit(data)\n",
    "new_colors = kmeans.cluster_centers_[kmeans.predict(data)]\n",
    "\n",
    "plot_pixels(data, colors=new_colors,\n",
    "            title=\"Reduced color space: 16 colors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c457d3",
   "metadata": {},
   "source": [
    "The result is a recoloring of the original pixels, where each pixel is assigned the color of its closest cluster center. Plotting these new colors in the image space rather than the pixel space shows us the effect of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5235f79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "china_recolored = new_colors.reshape(china.shape)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6),\n",
    "          subplot_kw=dict(xticks=[], yticks=[]))\n",
    "fig.subplots_adjust(wspace=0.05)\n",
    "ax[0].imshow(china)\n",
    "ax[0].set_title('Original Image', size=16)\n",
    "ax[1].imshow(china_recolored)\n",
    "ax[1].set_title('16-color Image', size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124dcec2",
   "metadata": {},
   "source": [
    "Some detail is certainly lost in the right panel, but the overall image is still easily recognizable. The image on the right achieves a compression factor of around 1 million! While this is an interesting application of k-means, there are certainly better way to compress information in images. This example shows the power of thinking outside of the box with unsupervised methods like k-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29099ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
