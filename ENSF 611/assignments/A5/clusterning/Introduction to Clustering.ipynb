{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning - Clustering\n",
    "\n",
    "The goal of clustering:\n",
    "> ... clustering is the task of partitioning the dataset into groups, called clusters. The goal is to split up the data in such a way that points within a single cluster are very similar and points in different clusters are different. Similarly to classification algorithms, clustering algorithms assign (or predict) a number to each data point, indicating which cluster a particular point belongs to.\n",
    "\n",
    "A big part is interpreting the clusters. You need to know what the features represent, where your data comes from.\n",
    "\n",
    "## KMeans Clustering\n",
    "\n",
    "Follow:\n",
    "- _Introduction to Machine Learning_ [Chapter 3](https://github.com/amueller/introduction_to_ml_with_python/blob/master/03-unsupervised-learning.ipynb) **Section 3.5.1 k-Means Clustering** \n",
    "- _Practical Statistics for Data Scientists_ [Chapter 7](https://github.com/gedeck/practical-statistics-for-data-scientists/blob/master/python/notebooks/Chapter%207%20-%20Unsupervised%20Learning.ipynb) K-Means algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does KMeans Clustering work?\n",
    "\n",
    "**The number of clusters has to be chosen by the user first**\n",
    "\n",
    ">The algorithm alternates between two steps: assigning each data point to the closest cluster center, and then setting each cluster center as the mean of the data points that are assigned to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_kmeans_algorithm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is randomness involved -> set random_state for consistent results.\n",
    "\n",
    "scikit-learn performs multiple clusterings `n_init=10` by default and returns the *best*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# generate synthetic two-dimensional data\n",
    "X, y = make_blobs(random_state=1)\n",
    "\n",
    "# build the clustering model\n",
    "kmeans = KMeans(n_clusters=3, random_state=4, n_init=10)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cluster memberships:\\n{}\".format(kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cluster memberships:\\n{}\".format(kmeans.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing clusters and cluster centres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], kmeans.labels_, markers='o')\n",
    "mglearn.discrete_scatter(\n",
    "    kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], [0, 1, 2],\n",
    "    markers='^', markeredgewidth=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another way to interpret Clustering\n",
    "\n",
    "Code from Practical Statistics for Data Scientists [Unsupervised Learning](https://github.com/gedeck/practical-statistics-for-data-scientists/blob/master/python/notebooks/Chapter%207%20-%20Unsupervised%20Learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a DataFrame with features and predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X, columns=['f1', 'f2'])\n",
    "df['label'] = kmeans.labels_\n",
    "df['label'] = df['label'].astype('category') #seaborn plots will be nicer\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a DataFrame with cluster centres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = pd.DataFrame(kmeans.cluster_centers_, columns=['f1', 'f2'])\n",
    "centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Plot features and cluster centres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "ax = sns.scatterplot(x='f1', y='f2', hue='label', ax=ax, data=df)\n",
    "\n",
    "centers.plot.scatter(x='f1', y='f2', ax=ax, marker='x', s=80, color='black');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Check cluster balances\n",
    "Small clusters might be noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(kmeans.labels_)\n",
    "print(counts)\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(counts.values(), labels=[f'Cluster {i}' for i in counts.keys()], autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "ax1.axis('equal');  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "ax1.set_title('Cluster size distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Plot means of the features in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(kmeans.n_clusters, 1, figsize=(6, 6), sharex=True)\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    center = centers.loc[i, :]\n",
    "    maxPC = 1.01 * np.max(np.max(np.abs(center)))\n",
    "    colors = ['C0' if l>0 else 'C1' for l in center]\n",
    "    ax.axhline(color='#888888')\n",
    "    center.plot.bar(ax=ax, color=colors)\n",
    "    ax.set_ylabel(f'Cluster{i}')\n",
    "    ax.set_ylim(-maxPC, maxPC)\n",
    "    if i == 0:\n",
    "        ax.set_title('Cluster centers per feature on original scale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Interpret clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could interpret and say:\n",
    "\n",
    "- Cluster 2 contains samples with large negative values in both features\n",
    "- Cluster 0 contains samples with large positive f2 values.\n",
    "- Cluster 1 contains samples with large negative f1 values.\n",
    "\n",
    "This would make more sense if we would know what the features f1 and f2 represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wine dataset\n",
    "\n",
    "Because k-means clustering uses distances, we scale the features using a `StandardScaler()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "X, _ = load_wine(return_X_y=True, as_frame=True)\n",
    "X_scaled = X.copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_scaled)\n",
    "X_scaled = pd.DataFrame(scaler.transform(X_scaled), columns=X_scaled.columns)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means with 3 clusters (arbitrary choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=54, n_init=10)\n",
    "kmeans.fit(X_scaled)\n",
    "# y_kmeans = kmeans.predict(df_scaled)\n",
    "X['clusters'] = kmeans.labels_\n",
    "X['clusters'] = X['clusters'].astype('category') #makes seaborn use qualitative color palette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster centers in orginal and scaled spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_), columns=X_scaled.columns)\n",
    "centers_scaled = pd.DataFrame(kmeans.cluster_centers_, columns=X_scaled.columns)\n",
    "centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize clusters and centers\n",
    "\n",
    "We have more than two dimensions:\n",
    "1. We choose a single pair\n",
    "2. We use PCA to reduce the dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "ax = sns.scatterplot(x='alcohol', y='color_intensity', hue='clusters', ax=ax, data=X)\n",
    "\n",
    "centers.plot.scatter(x='alcohol', y='color_intensity', ax=ax, marker='x', s=80, color='black');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_scaled)\n",
    "X_2D = pca.transform(X_scaled)\n",
    "sns.scatterplot(x=X_2D[:,0], y=X_2D[:,1], hue=X['clusters'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(kmeans.labels_)\n",
    "print(counts)\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(counts.values(), labels=[f'Cluster {i}' for i in counts.keys()], autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "ax1.axis('equal');  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "ax1.set_title('Cluster size distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Values of cluster centres in scaled space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(kmeans.n_clusters, 1, figsize=(6, 6), sharex=True)\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    center = centers_scaled.loc[i, :]\n",
    "    maxPC = 1.01 * np.max(np.max(np.abs(center)))\n",
    "    colors = ['C0' if l>0 else 'C1' for l in center]\n",
    "    ax.axhline(color='#888888')\n",
    "    center.plot.bar(ax=ax, color=colors)\n",
    "    ax.set_ylabel(f'Cluster{i}')\n",
    "    ax.set_ylim(-maxPC, maxPC)\n",
    "    if i == 0:\n",
    "        ax.set_title('Cluster centers per feature on standard scale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it is in scaled space, the above plots are with respect to the average value\n",
    "\n",
    "Cluster 0: High alcohol, medium acidity and medium color intensity wines\n",
    "\n",
    "Cluster 1: Medium alcohol, high acidity with high color intensity wines\n",
    "\n",
    "Cluster 2: Low alcohol, low acidity with low color intensity wines\n",
    "\n",
    "Another approach would be to group the features and comment on the feature groups. This requires domain knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
